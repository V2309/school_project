# agent_core.py (Phi√™n b·∫£n t·ªëi ∆∞u h√≥a truy xu·∫•t PDF)

import os
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
# S·ª≠ d·ª•ng LLM v√† Embedding c·ªßa Google
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
# Import c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt ƒë·ªÉ x√¢y d·ª±ng Agent
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.tools.retriever import create_retriever_tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.tools import tool

import json

# --- H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng ---
def preprocess_text(text):
    """L√†m s·∫°ch v√† chu·∫©n h√≥a vƒÉn b·∫£n t·ª´ PDF"""
    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† kho·∫£ng tr·∫Øng th·ª´a
    text = re.sub(r'\s+', ' ', text)  # Nhi·ªÅu kho·∫£ng tr·∫Øng th√†nh 1
    text = re.sub(r'\n+', '\n', text)  # Nhi·ªÅu xu·ªëng d√≤ng th√†nh 1
    
    # Lo·∫°i b·ªè header/footer th∆∞·ªùng g·∫∑p
    text = re.sub(r'Trang \d+', '', text)
    text = re.sub(r'Page \d+', '', text)
    
    # Chu·∫©n h√≥a d·∫•u c√¢u
    text = re.sub(r'\s+([.,;:])', r'\1', text)
    
    return text.strip()

# --- H√†m load documents ƒë∆∞·ª£c t·ªëi ∆∞u ---
def load_documents(sources):
    docs = []
    temp_files = []
    try:
        for source in sources:
            if isinstance(source, str):
                loader = WebBaseLoader(source)
                docs.extend(loader.load())
            else:
                temp_file_path = os.path.join(".", source.name)
                with open(temp_file_path, "wb") as f:
                    f.write(source.getbuffer())
                temp_files.append(temp_file_path)
                
                # S·ª≠ d·ª•ng PyPDFLoader v·ªõi extract_images=False ƒë·ªÉ tƒÉng t·ªëc
                loader = PyPDFLoader(temp_file_path, extract_images=False)
                loaded_docs = loader.load()
                
                # Ti·ªÅn x·ª≠ l√Ω v√† th√™m metadata cho m·ªói document
                for i, doc in enumerate(loaded_docs):
                    doc.page_content = preprocess_text(doc.page_content)
                    doc.metadata.update({
                        'source_file': source.name,
                        'page_number': i + 1,
                        'total_pages': len(loaded_docs),
                        'content_length': len(doc.page_content)
                    })
                docs.extend(loaded_docs)
    finally:
        for f in temp_files:
            if os.path.exists(f):
                os.remove(f)
    return docs

def split_documents(documents):
    """Chia nh·ªè documents v·ªõi chi·∫øn l∆∞·ª£c th√¥ng minh h∆°n"""
    # S·ª≠ d·ª•ng separators t·ªëi ∆∞u cho vƒÉn b·∫£n ti·∫øng Vi·ªát v√† h·ªçc thu·∫≠t
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,  # Gi·∫£m chunk size ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c
        chunk_overlap=150,  # TƒÉng overlap ƒë·ªÉ b·∫£o to√†n ng·ªØ c·∫£nh
        length_function=len,
        separators=[
            "\n\n",      # Paragraph breaks
            "\n",        # Line breaks  
            ". ",        # Sentence ends
            "? ",        # Question marks
            "! ",        # Exclamation marks
            "; ",        # Semicolons
            ", ",        # Commas
            " ",         # Spaces
            ""           # Characters
        ],
        add_start_index=True  # Th√™m start index ƒë·ªÉ tracking
    )
    
    chunks = text_splitter.split_documents(documents)
    
    # L·ªçc b·ªè chunks qu√° ng·∫Øn ho·∫∑c ch·ªâ ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát
    filtered_chunks = []
    for chunk in chunks:
        content = chunk.page_content.strip()
        if len(content) > 50 and len(content.split()) > 5:  # √çt nh·∫•t 50 k√Ω t·ª± v√† 5 t·ª´
            # Th√™m metadata v·ªÅ v·ªã tr√≠ chunk trong document
            chunk.metadata.update({
                'chunk_length': len(content),
                'word_count': len(content.split())
            })
            filtered_chunks.append(chunk)
    
    return filtered_chunks




# --- H√†m t·∫°o vector store t·ªëi ∆∞u v·ªõi hybrid search ---
def create_vector_store(text_chunks):
    """T·∫°o vector store v·ªõi embedding model t·ªët h∆°n"""
    # S·ª≠ d·ª•ng OpenAI embedding model m·ªõi nh·∫•t v√† t·ªët nh·∫•t
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        dimensions=1536,  # ƒê·∫£m b·∫£o consistency
        show_progress_bar=True
    )
    
    # T·∫°o FAISS vector store
    vector_store = FAISS.from_documents(
        documents=text_chunks, 
        embedding=embeddings
    )
    
    return vector_store

def create_hybrid_retriever(vector_store, text_chunks):
    """T·∫°o hybrid retriever k·∫øt h·ª£p vector search v√† keyword search"""
    
    # 1. Vector retriever v·ªõi MMR
    vector_retriever = vector_store.as_retriever(
        search_type="mmr",
        search_kwargs={
            'k': 4,
            'fetch_k': 15,
            'lambda_mult': 0.8,  # C√¢n b·∫±ng gi·ªØa relevance v√† diversity
        }
    )
    
    try:
        # 2. BM25 retriever cho keyword search
        bm25_retriever = BM25Retriever.from_documents(
            text_chunks,
            k=4
        )
        
        # 3. Ensemble retriever k·∫øt h·ª£p c·∫£ hai
        ensemble_retriever = EnsembleRetriever(
            retrievers=[vector_retriever, bm25_retriever],
            weights=[0.7, 0.3]  # ∆Øu ti√™n vector search h∆°n
        )
        
        return ensemble_retriever
        
    except ImportError:
        # Fallback v·ªÅ vector retriever n·∫øu kh√¥ng c√≥ rank_bm25
        print("Warning: rank_bm25 not installed. Using vector search only.")
        return vector_retriever


def get_generation_llm():
    """Kh·ªüi t·∫°o v√† tr·∫£ v·ªÅ m·ªôt LLM c·ªßa Google cho c√°c t√°c v·ª• t·∫°o n·ªôi dung."""
    try:
        # C·∫•u h√¨nh t·ªëi ∆∞u cho essay generation - s·ª≠ d·ª•ng model ·ªïn ƒë·ªãnh h∆°n
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",  # S·ª≠ d·ª•ng model theo y√™u c·∫ßu
            temperature=0.2,      # Gi·∫£m temperature ƒë·ªÉ output ·ªïn ƒë·ªãnh h∆°n
            max_tokens=4096,      # Gi·ªØ max_tokens v·ª´a ph·∫£i ƒë·ªÉ tr√°nh quota issues
            top_p=0.7,           # Gi·∫£m top_p ƒë·ªÉ t·∫≠p trung h∆°n
            max_retries=3,       # TƒÉng s·ªë l·∫ßn th·ª≠ l·∫°i
            timeout=90           # TƒÉng timeout cho response d√†i
        )
        print("[DEBUG] Generation LLM initialized successfully")
        return llm
    except Exception as e:
        print(f"[ERROR] Failed to initialize Generation LLM: {e}")
        raise

# --- H√ÄM M·ªöI: Logic t·∫°o c√¢u h·ªèi t·ª± lu·∫≠n ---
async def generate_essay_questions_logic(llm, prompt_template_str, num_questions, context=None, topic=None):
    """
    H√†m logic (async) ƒë·ªÉ g·ªçi LLM v√† t·∫°o c√¢u h·ªèi t·ª± lu·∫≠n v·ªõi x·ª≠ l√Ω l·ªói c·∫£i thi·ªán.
    """
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            prompt = None
            if topic:
                # Tr∆∞·ªùng h·ª£p 2: T·∫°o theo ch·ªß ƒë·ªÅ
                prompt = prompt_template_str.format(num_questions=num_questions, topic=topic)
            elif context:
                # Tr∆∞·ªùng h·ª£p 1: T·∫°o theo file (RAG)
                max_context_len = 12000  # Gi·∫£m t·ª´ 15000 ƒë·ªÉ tr√°nh v∆∞·ª£t token limit
                if len(context) > max_context_len:
                    context = context[:max_context_len] + "\n... (N·ªôi dung ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn)"
                    print(f"[DEBUG] Context truncated to {max_context_len} chars for essay generation")
                    
                prompt = prompt_template_str.format(num_questions=num_questions, context=context)
            
            if not prompt:
                raise ValueError("Thi·∫øu context ho·∫∑c topic ƒë·ªÉ t·∫°o c√¢u h·ªèi.")

            print(f"[DEBUG] Invoking essay generation chain (num_questions: {num_questions}, retry: {retry_count + 1})...")
            
            # TƒÉng timeout v√† max_tokens cho response d√†i
            response = await llm.ainvoke(prompt)
            
            raw_response = response.content if hasattr(response, 'content') else str(response)
            
            print(f"[DEBUG] Raw response length: {len(raw_response)} chars")
            print(f"[DEBUG] Raw response preview: {raw_response[:300]}...")
            
            # C·∫£i thi·ªán logic tr√≠ch xu·∫•t JSON v·ªõi nhi·ªÅu ph∆∞∆°ng ph√°p
            json_str = extract_json_from_response(raw_response)
            
            if not json_str:
                print("[ERROR] No valid JSON found in LLM response.")
                if len(raw_response) < 100:
                    print(f"[ERROR] Full response was: {raw_response}")
                else:
                    print(f"[ERROR] Response preview: {raw_response[:500]}...")
                    print(f"[ERROR] Response end: ...{raw_response[-500:]}")
                raise ValueError("Kh√¥ng t√¨m th·∫•y n·ªôi dung JSON h·ª£p l·ªá trong ph·∫£n h·ªìi c·ªßa AI.")
                
            # Parse JSON v·ªõi x·ª≠ l√Ω l·ªói t·ªët h∆°n
            parsed_json = parse_json_safely(json_str)
            
            if 'questions' not in parsed_json or not isinstance(parsed_json['questions'], list):
                print("[ERROR] JSON output is missing 'questions' list.")
                raise ValueError("ƒê·ªãnh d·∫°ng JSON t·ª´ AI kh√¥ng h·ª£p l·ªá (thi·∫øu key 'questions').")
            
            questions = parsed_json['questions']
            if len(questions) != num_questions:
                print(f"[WARNING] Expected {num_questions} questions, got {len(questions)}")
                
            print(f"[DEBUG] Successfully parsed JSON with {len(questions)} questions")
            return questions
        except json.JSONDecodeError as jde:
            retry_count += 1
            print(f"[ERROR] JSON decode error on attempt {retry_count}: {jde}")
            if retry_count >= max_retries:
                print(f"[ERROR] Failed after {max_retries} attempts")
                raise ValueError(f"L·ªói khi ƒë·ªçc ƒë·ªãnh d·∫°ng JSON t·ª´ AI sau {max_retries} l·∫ßn th·ª≠: {jde}. JSON c√≥ th·ªÉ ƒë√£ b·ªã c·∫Øt c·ª•t ho·∫∑c kh√¥ng h·ª£p l·ªá.")
            print(f"[INFO] Retrying... ({retry_count}/{max_retries})")
            continue
            
        except Exception as e:
            retry_count += 1
            print(f"[ERROR] Error in generate_essay_questions_logic on attempt {retry_count}: {e}")
            if retry_count >= max_retries:
                print(f"[ERROR] Failed after {max_retries} attempts")
                import traceback
                traceback.print_exc()
                raise
            print(f"[INFO] Retrying due to error... ({retry_count}/{max_retries})")
            continue
            
    # If we get here, all retries failed
    raise ValueError(f"Kh√¥ng th·ªÉ t·∫°o c√¢u h·ªèi sau {max_retries} l·∫ßn th·ª≠.")


def extract_json_from_response(raw_response):
    """
    Tr√≠ch xu·∫•t JSON t·ª´ response v·ªõi nhi·ªÅu ph∆∞∆°ng ph√°p kh√°c nhau.
    """
    # Ph∆∞∆°ng ph√°p 1: T√¨m JSON block v·ªõi ```json
    json_match = re.search(r'```json\s*({[\s\S]*?})\s*```', raw_response, re.DOTALL)
    if json_match:
        json_str = json_match.group(1).strip()
        print(f"[DEBUG] Found JSON in ```json block, length: {len(json_str)}")
        return json_str
    
    # Ph∆∞∆°ng ph√°p 2: T√¨m JSON th√¥ v·ªõi c√¢n b·∫±ng d·∫•u ngo·∫∑c
    json_str = extract_balanced_json(raw_response)
    if json_str:
        print(f"[DEBUG] Found balanced JSON object, length: {len(json_str)}")
        return json_str
    
    # Ph∆∞∆°ng ph√°p 3: T√¨m JSON c∆° b·∫£n (fallback)
    start_idx = raw_response.find('{')
    end_idx = raw_response.rfind('}')
    if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
        json_str = raw_response[start_idx:end_idx+1].strip()
        print(f"[DEBUG] Found basic JSON object, length: {len(json_str)}")
        return json_str
    
    return None


def extract_balanced_json(text):
    """
    Tr√≠ch xu·∫•t JSON b·∫±ng c√°ch c√¢n b·∫±ng d·∫•u ngo·∫∑c nh·ªçn v·ªõi x·ª≠ l√Ω truncation.
    """
    start_idx = text.find('{')
    if start_idx == -1:
        return None
    
    bracket_count = 0
    in_string = False
    escape_next = False
    last_valid_end = -1
    
    for i, char in enumerate(text[start_idx:], start_idx):
        if escape_next:
            escape_next = False
            continue
            
        if char == '\\' and in_string:
            escape_next = True
            continue
            
        if char == '"' and not escape_next:
            in_string = not in_string
            continue
            
        if not in_string:
            if char == '{':
                bracket_count += 1
            elif char == '}':
                bracket_count -= 1
                if bracket_count == 0:
                    return text[start_idx:i+1].strip()
                elif bracket_count > 0:
                    # L∆∞u l·∫°i v·ªã tr√≠ h·ª£p l·ªá cu·ªëi c√πng
                    last_valid_end = i
    
    # N·∫øu JSON b·ªã c·∫Øt c·ª•t, th·ª≠ tr·∫£ v·ªÅ ph·∫ßn c√≥ th·ªÉ s·ª≠a ƒë∆∞·ª£c
    if bracket_count > 0 and start_idx != -1:
        # Tr·∫£ v·ªÅ ph·∫ßn JSON cho ƒë·∫øn cu·ªëi text
        return text[start_idx:].strip()
    
    return None


def parse_json_safely(json_str):
    """
    Parse JSON v·ªõi x·ª≠ l√Ω l·ªói an to√†n v√† c·ªë g·∫Øng s·ª≠a ch·ªØa.
    """
    try:
        # Th·ª≠ parse JSON b√¨nh th∆∞·ªùng
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"[DEBUG] Initial JSON parse failed: {e}")
        
        # Th·ª≠ s·ª≠a ch·ªØa JSON b·ªã c·∫Øt c·ª•t
        fixed_json = attempt_json_repair(json_str)
        if fixed_json:
            try:
                result = json.loads(fixed_json)
                print(f"[DEBUG] Successfully repaired and parsed JSON")
                return result
            except json.JSONDecodeError:
                print(f"[DEBUG] JSON repair failed")
        
        # N·∫øu kh√¥ng s·ª≠a ƒë∆∞·ª£c, raise l·ªói g·ªëc
        raise e


def attempt_json_repair(json_str):
    """
    C·ªë g·∫Øng s·ª≠a ch·ªØa JSON b·ªã c·∫Øt c·ª•t ho·∫∑c c√≥ l·ªói nh·ªè v·ªõi c√°ch ti·∫øp c·∫≠n ƒë∆°n gi·∫£n.
    """
    try:
        json_str = json_str.strip()
        original_len = len(json_str)
        
        # B∆∞·ªõc 1: X·ª≠ l√Ω string b·ªã c·∫Øt c·ª•t
        if json_str.count('"') % 2 == 1:  # S·ªë l·∫ª quotes = string ch∆∞a ƒë√≥ng
            json_str += '"'
            print(f"[DEBUG] Closed unterminated string")
        
        # B∆∞·ªõc 2: ƒê√≥ng c√°c bracket b·ªã thi·∫øu
        open_braces = json_str.count('{')
        close_braces = json_str.count('}')
        if open_braces > close_braces:
            missing_braces = open_braces - close_braces
            json_str += '}' * missing_braces
            print(f"[DEBUG] Added {missing_braces} missing closing braces")
        
        # B∆∞·ªõc 3: X·ª≠ l√Ω JSON structure c∆° b·∫£n
        # ƒê·∫£m b·∫£o kh√¥ng c√≥ trailing comma
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
        
        # N·∫øu v·∫´n l·ªói, th·ª≠ m·ªôt c√°ch ti·∫øp c·∫≠n kh√°c
        try:
            json.loads(json_str)  # Test parse
            if len(json_str) != original_len:
                print(f"[DEBUG] JSON repaired: {original_len} -> {len(json_str)} chars")
            return json_str
        except json.JSONDecodeError:
            # Fallback: T·∫°o JSON structure t·ªëi thi·ªÉu
            return create_fallback_json()
        
    except Exception as e:
        print(f"[DEBUG] JSON repair attempt failed: {e}")
        return create_fallback_json()

def create_fallback_json():
    """T·∫°o JSON fallback khi kh√¥ng s·ª≠a ƒë∆∞·ª£c."""
    fallback = {
        "questions": [
            {
                "question_number": 1,
                "question_text": "C√¢u h·ªèi kh√¥ng th·ªÉ ƒë∆∞·ª£c t·∫°o do l·ªói parsing. Vui l√≤ng th·ª≠ l·∫°i.",
                "suggested_answer": "Xin l·ªói, ƒë√£ c√≥ l·ªói trong qu√° tr√¨nh t·∫°o c√¢u h·ªèi."
            }
        ]
    }
    print("[DEBUG] Using fallback JSON structure")
    return json.dumps(fallback, ensure_ascii=False)
# --- T·∫†O AGENT S·ª¨ D·ª§NG GEMINI V·ªöI RETRIEVER T·ªêI ∆ØU ---
def create_agent_executor(vector_store, system_prompt_str, text_chunks=None):
    """T·∫°o agent executor v·ªõi retrieval ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a"""
    
    # T·∫°o hybrid retriever thay v√¨ retriever ƒë∆°n gi·∫£n
    if text_chunks:
        retriever = create_hybrid_retriever(vector_store, text_chunks)
    else:
        # Fallback v·ªÅ retriever th√¥ng th∆∞·ªùng n·∫øu kh√¥ng c√≥ text_chunks
        retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                'k': 5,
                'fetch_k': 20,
                'lambda_mult': 0.75,
            }
        )

    # 1. C√îNG C·ª§ 1: Document search v·ªõi m√¥ t·∫£ ƒë∆∞·ª£c c·∫£i thi·ªán
    document_search_tool = create_retriever_tool(
        retriever,
        "document_search",
        """‚≠ê C√îNG C·ª§ QUAN TR·ªåNG NH·∫§T - LU√îN D√ôNG TR∆Ø·ªöC TI√äN ‚≠ê
        
        T√¨m ki·∫øm th√¥ng tin trong t√†i li·ªáu h·ªçc thu·∫≠t ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n (PDF, slide, gi√°o tr√¨nh).
        
        üö® QUY T·∫ÆC B·∫ÆT BU·ªòC:
        - PH·∫¢I s·ª≠ d·ª•ng TR∆Ø·ªöC TI√äN cho M·ªåI c√¢u h·ªèi v·ªÅ ki·∫øn th·ª©c, kh√°i ni·ªám, ƒë·ªãnh nghƒ©a
        - PH·∫¢I s·ª≠ d·ª•ng cho M·ªåI c√¢u h·ªèi h·ªçc thu·∫≠t, d√π ƒë∆°n gi·∫£n hay ph·ª©c t·∫°p  
        - KH√îNG ƒê∆Ø·ª¢C b·ªè qua c√¥ng c·ª• n√†y v·ªõi b·∫•t k·ª≥ l√Ω do g√¨
        
        üéØ S·ª≠ d·ª•ng khi:
        - C√¢u h·ªèi v·ªÅ ƒë·ªãnh nghƒ©a, kh√°i ni·ªám, l√Ω thuy·∫øt
        - Y√™u c·∫ßu gi·∫£i th√≠ch n·ªôi dung b√†i h·ªçc
        - H·ªèi v·ªÅ c√¥ng th·ª©c, quy tr√¨nh, ph∆∞∆°ng ph√°p
        - B·∫•t k·ª≥ th√¥ng tin n√†o c√≥ th·ªÉ xu·∫•t hi·ªán trong t√†i li·ªáu
        
        C√¥ng c·ª• s·ª≠ d·ª•ng hybrid search (vector + keyword) ƒë·ªÉ t√¨m ki·∫øm ch√≠nh x√°c nh·∫•t."""
    )

    # 2. C√îNG C·ª§ 2: Web search cho th√¥ng tin b·ªï sung - CH·ªà KHI DOCUMENT_SEARCH TH·∫§T B·∫†I
    web_search_tool = TavilySearchResults(
        k=3, 
        name="web_search",
        description="""üö´ CH·ªà S·ª¨ D·ª§NG SAU KHI DOCUMENT_SEARCH TH·∫§T B·∫†I üö´
        
        ‚ö†Ô∏è KH√îNG ƒê∆Ø·ª¢C d√πng l√†m c√¥ng c·ª• ƒë·∫ßu ti√™n cho c√¢u h·ªèi h·ªçc thu·∫≠t
        ‚ö†Ô∏è CH·ªà d√πng KHI document_search kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan
        
        H·ªØu √≠ch cho:
        - Tin t·ª©c m·ªõi, c·∫≠p nh·∫≠t g·∫ßn ƒë√¢y  
        - Th√¥ng tin ngo√†i ph·∫°m vi t√†i li·ªáu ƒë√£ t·∫£i
        - Ch·ªß ƒë·ªÅ kh√¥ng c√≥ trong gi√°o tr√¨nh/slide"""
    )

   

    # 4. T·∫≠p h·ª£p c√°c c√¥ng c·ª• - ƒê·∫∂T DOCUMENT_SEARCH TOOL ƒê·∫¶U TI√äN ƒê·ªÇ ∆ØU TI√äN
    tools = [document_search_tool, web_search_tool]

    # 5. Gemini model v·ªõi c·∫•u h√¨nh ·ªïn ƒë·ªãnh h∆°n
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", 
            temperature=0.05,  # Gi·∫£m temperature r·∫•t th·∫•p ƒë·ªÉ tu√¢n th·ªß quy t·∫Øc nghi√™m ng·∫∑t
            max_tokens=4096,  # TƒÉng l√™n ƒë·ªÉ ƒë·ªß cho n·ªôi dung d√†i (9 ph·∫ßn h∆∞·ªõng d·∫´n h·ªçc t·∫≠p)
            top_p=0.7,  # Gi·∫£m top_p ƒë·ªÉ focused h∆°n
            max_retries=3,  # Retry n·∫øu API call fail
            request_timeout=60  # Timeout 60s cho API calls
        )
        print("[DEBUG] Gemini LLM initialized successfully")
    except Exception as e:
        print(f"[ERROR] Failed to initialize Gemini LLM: {e}")
        raise
    
    # 6. Prompt template ƒë∆∞·ª£c c·∫£i thi·ªán
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt_str),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ])

    # 7. T·∫°o Agent v·ªõi error handling t·ªët h∆°n
    agent = create_tool_calling_agent(llm, tools, prompt)

    # 8. Agent Executor v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u v√† error handling
    try:
        agent_executor = AgentExecutor(
            agent=agent, 
            tools=tools, 
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=10,  # TƒÉng s·ªë l·∫ßn th·ª≠ ƒë·ªÉ ƒë·∫£m b·∫£o d√πng tools
            max_execution_time=90,  # TƒÉng timeout
            return_intermediate_steps=False,
            early_stopping_method="force"  # Thay ƒë·ªïi ƒë·ªÉ √©p s·ª≠ d·ª•ng tools nhi·ªÅu h∆°n
        )
        print(f"[DEBUG] Agent Executor created successfully with {len(tools)} tools")
    except Exception as e:
        print(f"[ERROR] Failed to create Agent Executor: {e}")
        raise
    
    return agent_executor